use tokenizers::models::bpe::BPE;
use tokenizers::models::bpe::BpeTrainer;
use tokenizers::{AddedToken, DecoderWrapper, NormalizerWrapper, PostProcessorWrapper, PreTokenizerWrapper, Tokenizer, TokenizerImpl};
use anyhow::{Error};
use log::{ info};
use env_logger;

fn main() -> Result<(), Error> {
    // åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    env_logger::init();

    // åˆ›å»ºæ–°çš„BPEåˆ†è¯å™¨
    let tokenizer = BPE::builder().unk_token("[UNK]".to_string()).build()
        .map_err(|e| anyhow::anyhow!(e))?;

    let mut tokenizer: TokenizerImpl<
        BPE,
        NormalizerWrapper,
        PreTokenizerWrapper,
        PostProcessorWrapper,
        DecoderWrapper,
    > = TokenizerImpl::new(tokenizer);

    // å®šä¹‰ç‰¹æ®Š token å¹¶åˆ›å»ºè®­ç»ƒå™¨
    let mut trainer = BpeTrainer::builder()
        .special_tokens(vec![
            AddedToken::from("[UNK]", true),
            AddedToken::from("[CLS]", true),
            AddedToken::from("[SEP]", true),
            AddedToken::from("[PAD]", true),
            AddedToken::from("[MASK]", true),
        ])
        .build();

    // è®­ç»ƒæ–‡ä»¶è·¯å¾„
    let files = vec![
        "data/wikitext/wiki.test.raw".into(),
    ];

    // ä½¿ç”¨æ–‡ä»¶è®­ç»ƒåˆ†è¯å™¨
    tokenizer.train_from_files(&mut trainer, files)
        .map_err(|e| anyhow::anyhow!(e))?;

    // ä¿å­˜åˆ†è¯å™¨
    tokenizer.save("quickstart/models/tokenizer-wiki.json", false)
        .map_err(|e| anyhow::anyhow!(e))?;
    println!("åˆ†è¯å™¨å·²ä¿å­˜åˆ° quickstart/models/tokenizer-wiki.json");
    // ä»æ–‡ä»¶åŠ è½½åˆ†è¯å™¨
    let tokenizer = Tokenizer::from_file("quickstart/models/tokenizer-wiki.json")
        .map_err(|e| anyhow::anyhow!(e))?;

    // å¯¹è¾“å…¥è¿›è¡Œç¼–ç 
    let encoder_result = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?", true)
        .map_err(|e| anyhow::anyhow!(e))?;

    // è¾“å‡ºç¼–ç ç»“æœ
    println!("ç¼–ç ç»“æœ: {:?}", encoder_result);
    let decode_result = tokenizer.decode(encoder_result.get_ids(), false);
    println!("è§£ç ç»“æœ: {:?}", decode_result);
    Ok(())
}
